---
category: "[[Hypothesis Assessment - Outcome]]"
source_url: "https://en.wikipedia.org/wiki/Extension_neglect"
description: "Occurs where the quantity of the sample size is not sufficiently taken into consideration when assessing the outcome relevance or judgement"
---

# 1. ELI5 (Explain Like I'm 5)

Imagine you flip a coin 3 times and it lands on heads every time. You might think, "Wow! This coin must be magic because it always lands on heads!" ü™ô But what if you only flipped it 3 times? That's not very many tries to be sure!

Extension neglect is when people make big decisions based on only a few examples, without thinking about how many times they actually tested something. They forget that having more information (like flipping the coin 100 times instead of 3) would give them a much better idea of what's really true.

# 2. Recognizing the Bias

## 2.1. Key Signs to Watch For

**Small Sample Confidence**: You're making strong conclusions after only testing something a few times or with very few people.

**Case Study Overemphasis**: You give too much importance to individual stories or examples while ignoring larger patterns.

 **Scope Blindness**: You don't stop to ask "How many times have we actually tested this?" or "What's the sample size?"

 **Pattern Jumping**: You immediately see patterns in small amounts of data that might just be random coincidence.

 **Scale Insensitivity**: You treat results from tiny groups the same as results from large, comprehensive studies.

üé≤ **Probability Ignorance**: You don't consider how likely your results would be just by random chance.

# 3. How It Shows Up

## 3.1. Workplace Examples

- **Pilot Project Premature Scaling**: A company runs a successful pilot program with 10 employees and immediately rolls it out to 1,000 people, not realizing the small group wasn't representative
- **Customer Feedback Overgeneralization**: A product team makes major changes based on feedback from 3 enthusiastic early adopters, ignoring that they might not represent the broader customer base
- **A/B Testing Impatience**: Marketing teams stop A/B tests after only a few hours of data because one version appears better, not waiting for statistically significant results
- **Interview Decision Bias**: Managers hire candidates based on impressive interviews with just 2-3 people, rather than using more comprehensive assessment methods

## 3.2. Daily Life Scenarios

- **Restaurant Review Snap Judgments**: People write glowing 5-star reviews after visiting a restaurant once, not considering they might have caught them on an unusually good day
- **Product Experience Generalization**: Someone tries a new skincare product for 3 days and declares it "miraculous" or "terrible," not giving it enough time to show real effects
- **Weather Pattern Assumptions**: After two unusually hot days in a row, people conclude "climate change is accelerating" or "this summer will be the hottest ever"
- **Social Media Trend Misinterpretation**: People see a few posts about something and assume it's a major trend, not realizing it might just be a small bubble of activity

## 3.3. Financial Decision-Making üí∞

- **Investment Pattern Chasing**: Investors see a stock go up 3 days in a row and buy heavily, thinking they've found a "pattern," not recognizing random market fluctuations
- **Business Success Story Mimicry**: Entrepreneurs copy strategies from 2-3 successful companies they've read about, not considering those might be exceptional cases
- **Real Estate Market Timing**: People see home prices rise in their neighborhood for 2 months and decide to "buy now before it's too late," ignoring longer-term trends
- **Cryptocurrency Hype**: Investors jump into new cryptocurrencies after seeing a few days of gains, not considering the small sample size and high volatility

# 4. Quick Test

## 4.1. Self-Assessment Questions

1. **The Sample Size Question**: When you made your last important decision, how many examples or how much data did you actually consider? Was it 3, 10, 100, or 1000? Be honest about whether your sample size was adequate.

2. **The Coin Flip Test**: If you flipped a coin 3 times and got heads each time, how confident would you be that it's not a fair coin? Now ask yourself: would you be more confident after 10 heads in a row? The difference in confidence shows how sample size affects judgment.

3. **The Randomness Challenge**: Could the pattern you're seeing reasonably occur by random chance? If you're not sure, you're probably neglecting to consider sample size and probability.

4. **The Representative Test**: How similar is your sample to the larger group you're making conclusions about? If you tested something only with college students, can you really apply those findings to the general population?

5. **The Time Frame Question**: Have you observed this pattern over a meaningful time period, or are you drawing conclusions from a very short observation window?

**Scoring**: If you frequently make decisions based on small samples, don't consider statistical significance, or struggle to assess whether patterns could be random, you're likely experiencing extension neglect.

# 5. How to Counteract

## 5.1. Ô∏è **Immediate Strategies**

### 5.1.1. **Sample Size Awareness**

- **Minimum sample setting**: Establish minimum sample sizes before starting any evaluation or test
- **Power thinking**: Always consider whether you have enough data to detect meaningful effects
- **Statistical literacy**: Learn basic statistical concepts like statistical significance and confidence intervals
- **Sample diversity check**: Ensure your sample represents the diversity of the group you're studying

### 5.1.2. **Cognitive Calibration Practices**

- **Probability estimation**: Practice estimating the likelihood that your observations could occur by random chance
- **Bayesian thinking**: Update your beliefs gradually as you accumulate more evidence, rather than jumping to conclusions
- **Confidence scaling**: Adjust your confidence level based on sample size‚Äîbe much more cautious with small samples
- **Temporal awareness**: Consider whether you've observed the phenomenon over a meaningful time period

## 5.2. **Systematic Approaches**

### 5.2.1. **Research And Testing Protocols**

- **Pre-determined sample sizes**: Set required sample sizes before beginning any research or testing
- **Statistical power analysis**: Calculate the sample size needed to detect meaningful effects
- **Random sampling methods**: Use proper random sampling techniques to ensure representative samples
- **Replication planning**: Plan to replicate findings with different samples and conditions

### 5.2.2. **Decision-Making Frameworks**

- **Multi-stage decision processes**: Make important decisions in stages, with each stage requiring more evidence
- **Confidence interval reporting**: Report results with confidence intervals rather than single point estimates
- **Error rate awareness**: Consider both Type I (false positive) and Type II (false negative) errors in your analysis
- **Meta-analysis thinking**: Look at multiple studies rather than relying on single results

## 5.3. **Workplace Implementation**

### 5.3.1. **For Leaders and Managers**

- **Statistical training**: Provide basic statistical training for team members involved in research and decision-making
- **Research standards**: Establish clear standards for sample sizes and testing protocols
- **Peer review processes**: Implement peer review for research findings and conclusions
- **Gradual rollout strategies**: Use phased rollouts rather than full implementation based on limited testing

### 5.3.2. **For Teams and Organizations**

- **Data literacy programs**: Build organization-wide understanding of data interpretation and statistics
- **Testing infrastructure**: Invest in proper A/B testing and experimental design capabilities
- **Documentation requirements**: Mandate documentation of sample sizes, methods, and limitations
- **Cross-validation practices**: Require validation of findings across different samples and contexts

### 5.3.3. **For Individual Contributors**

- **Statistical education**: Invest in learning basic statistics and research methods
- **Methodology improvement**: Focus on improving research and testing methodologies
- **Collaboration with experts**: Work with statisticians or research experts when designing studies
- **Continuous learning**: Stay updated on best practices in research and data analysis

## 5.4. **Personal Life Applications**

### 5.4.1. **Information Evaluation**

- **Source assessment**: Evaluate the sample size and methodology behind claims you encounter
- **Media literacy**: Develop skills to critically evaluate statistics and claims in news and media
- **Personal experimentation**: When testing things in your life, ensure adequate sample sizes and duration
- **Health decision-making**: Be cautious about health claims based on small studies or personal testimonials

### 5.4.2. **Decision-Making Improvement**

- **Evidence grading**: Learn to grade evidence based on sample size and methodological quality
- **Patience cultivation**: Practice waiting for more evidence before making important decisions
- **Multiple source consultation**: Always consult multiple sources before drawing conclusions
- **Probabilistic thinking**: Think in terms of probabilities rather than certainties

## 5.5. **Advanced Techniques**

### 5.5.1. **Statistical Competence Development**

- **Statistical significance understanding**: Learn to properly interpret p-values and statistical significance
- **Effect size awareness**: Focus on practical significance rather than just statistical significance
- **Confidence interval interpretation**: Understand how to interpret and use confidence intervals
- **Power analysis skills**: Learn to calculate and use statistical power in research design

### 5.5.2. **Research Design Excellence**

- **Experimental design mastery**: Learn proper experimental design techniques
- **Control group implementation**: Always include appropriate control groups when testing interventions
- **Randomization expertise**: Understand and implement proper randomization techniques
- **Blinding procedures**: Use single-blind and double-blind procedures when appropriate

## 5.6. **Progress Tracking Tools**

### 5.6.1. **Quality Control Systems**

- **Sample size calculators**: Use tools to calculate required sample sizes for different scenarios
- **Statistical software**: Utilize statistical software for proper data analysis
- **Research checklists**: Create and use checklists for research design and implementation
- **Methodology reviews**: Regular reviews of research methodologies and practices

### 5.6.2. **Learning And Development**

- **Statistics courses**: Take courses in statistics and research methodology
- **Research practice**: Engage in research projects to build practical experience
- **Expert consultation**: Consult with statisticians and research experts
- **Peer learning**: Learn from colleagues with expertise in research and statistics

### 5.6.3. **Decision Support Tools**

- **Decision matrices**: Use structured decision-making tools that account for uncertainty
- **Probability calculators**: Use tools to calculate probabilities and assess random chance
- **Confidence interval calculators**: Calculate and interpret confidence intervals properly
- **Meta-analysis tools**: Learn to conduct and interpret meta-analyses of multiple studies

**Remember**: Extension neglect is particularly dangerous in our world of information overload, where we're constantly exposed to small samples and anecdotes that can mislead us. The human brain is naturally drawn to patterns and stories, but we need to consciously consider whether we have enough evidence to support our conclusions.

**Key Insight**: The most reliable knowledge comes from large, representative samples collected over meaningful time periods. When you encounter a surprising finding or pattern, your first question should always be: "How much data is this based on, and is it enough to be meaningful?" Small samples can be useful for generating hypotheses, but never for confirming them.

